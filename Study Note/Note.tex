% default 12pt
\documentclass[12pt]{article}
 
% \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend,bm}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage[margin=0.75in]{geometry}
\usepackage{pgfpages}
% % For shrinking 4 pages to 1
% \pgfpagesuselayout{4 on 1}[a4paper, border shrink=0mm]
% 2 in 1
% \pgfpagesuselayout{2 on 1}[a4paper, border shrink=0mm, landscape]

\setlength{\parskip}{0em}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep = 0mm}

\theoremstyle{definition}
\newtheorem{definition}{DEFINITION}[subsection]


\pagestyle{fancy}

\let\newproof\proof
\renewenvironment{proof}{\begin{addmargin}[1em]{0em}\begin{newproof}}{\end{newproof}\end{addmargin}\qed}

\newtheorem{theorem}{THEOREM}[subsection]

\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\uline}[1]{\rule[0pt]{#1}{0.4pt}}
\newcommand{\trace}[1]{\text{tr}(#1)}
\newcommand{\Hom}{\text{Hom}}
\newcommand{\Maps}{\text{Maps}}
\newcommand{\image}{\text{im}}
\newcommand{\Mat}{\text{Mat}}
\newcommand{\suchthat}{\textit{ s.t. }}
\newcommand{\sgn}{\textbf{sgn}}
\newcommand{\adj}{\text{adj}}
\newcommand{\diag}{\text{diag}}
\newcommand{\transpose}[1]{#1^\mathsf{T}}
\newcommand{\Prob}[1]{\mathbb{P}(#1)}
\newcommand{\Expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\Var}[1]{\text{Var}\left[#1\right]}
\newcommand{\Cov}[1]{\text{Cov}\left[#1\right]}

%Set the course name here
\newcommand{\coursename}{Bayesian Theory}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}

\newtcbtheorem[number within=section]{mytheo}{Theorem}
{colback=red!5,colframe=red!35!black,fonttitle=\bfseries}{th}
\newtheorem{lemma}{LEMMA}[subsection]
\newtheorem{prop}{PROPOSITION}[subsection]
\newtheorem{corollary}{COROLLARY}[subsection]
\newtheorem{example}{EXAMPLE}[subsection]
 
\lhead{\coursename}
\rhead{Quick Notes}

\title{\coursename\\Quick Notes}
\author{Ian S.W. Ma}
\date{Spring 2020}

 
 
\begin{document}
\maketitle
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
\pagenumbering{gobble}
\tableofcontents
\newpage

\pagenumbering{arabic}
\setcounter{page}{1}

% Section 1
\section{Statistics Basics}
    \subsection*{General Structure: Probability Space/Triple}
    Probability Space/Triple $(\Omega, \mathcal{F}, \mathbb{P})$
    \begin{itemize}
        \item $\Omega$: \textbf{Sample Space}, set of all posible outcomes.
        \item $\mathcal{F}$: \textbf{Set of events \(\sigma\)}, each event is a subset of $\Omega$
        \item $\mathbb{P}$ Probability of event $\sigma \in \mathcal{F}$
    \end{itemize}
    \subsection*{Conditional Probability}
    Conditional probability of $A$ given $B$: $\Prob{A|B} = \frac{\Prob{A \cap B}}{\Prob{B}} \Leftrightarrow \Prob{A|B}\Prob{B} = \Prob{A \cap B}$
    \subsection*{General properties}
    \begin{itemize}
        \item $\Prob{A \cup B} = \Prob{A} + \Prob{B} - \Prob{A \cap B}$
            \subitem Events are mutually exclusive $\Rightarrow \Prob{A \cap B} = 0$
        \item $\Prob{A \cap B} = \Prob{A|B}\Prob{B}$
            \subitem Events are independent $\Rightarrow \Prob{A|B} = \Prob{A}$
        \item Law of Total Probability
            \subitem If events $A_1, A_2, A_N$ are mutually exclusive and exhaustive ($\bigcup_{i = 1}^N A_i = \Omega$) then:
            $$\Prob{B} = \sum_{i=1}^N \Prob{A_i \cap B} = \sum_{i=1}^N \Prob{B|A_i}\Prob{A_i}$$
    \end{itemize}
    \subsection*{Random Variables}
    \begin{tabular}{|c|c|c|}
        \hline
        $X \sim$ Distribution & $\Expect{X}$ & $\Var{X}$\\
        \hline
        Normal$(\mu, \sigma^2)$ & $\mu$ & $\sigma^2$\\
        \hline
        Bernoulli$(\theta)$ & $\theta$ & $\theta(1-\theta)$\\
        \hline
        Binomial$(\theta)$ & $n\theta$ & $n\theta(1-\theta)$\\
        \hline
        Poisson$(\mu)$ & $\mu$ & $\mu$\\
        \hline
        Uniform$(\alpha, \beta)$ & $\frac{\alpha + \beta}{2}$ & $\frac{(\alpha + \beta)^2}{12}$\\
        \hline
        Beta$(\alpha, \beta)$ & $\frac{\alpha}{\alpha + \beta}$ & $\frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta+1)}$\\
        \hline
        Gamma$(\alpha, \beta)$ & $\frac{\alpha}{\beta}$ & $\frac{\alpha}{\beta^2}$\\
        \hline
        Inv. Gamma$(\alpha, \beta)$ & $\frac{\beta}{\alpha -1}$ & $\frac{\beta^2}{(\alpha-1)^2(\alpha -2)}$\\
        \hline
        Exponential$(\lambda)$ & $\frac{1}{\lambda}$ & $\frac{1}{\lambda^2}$\\
        \hline
    \end{tabular}
    \subsection*{Random Vectors}
    \begin{tabular}{|c|c|c|c|}
        \hline
        $\mathbf{X} = X_1,...,X_k \sim$ Distribution & $\Expect{X_i}$ & $\Var{X_i}$ & $\Cov{X_i,X_j}$\\
        \hline
        Multivariative Normal$(\bm{\mu}, \Sigma)$ & $\mu_i$ & $\Sigma_{i,i}$ & $\Sigma_{i,j}$\\
        \hline
        Multinomial$(n,\theta_1,...,\theta_k)$ & $n\theta_i$ & $n(1-\theta_i)$ & $-n\theta_i\theta_j$\\
        \hline
    \end{tabular}


\pagebreak
\section{Bayes Throrem}
    \subsection*{Discrete Case}
    \begin{align*}
        \Prob{X=x_i| Y = y_j} &= \frac{\Prob{X = x_i, Y = y_j}}{\Prob{Y = y_j}}\\
        &= \frac{\Prob{Y = y_j|X=x_i} \Prob{X = x_i}}{\Prob{Y = y_j}}\\
        &= \frac{\text{Likelihood}\times \text{Prior}}{\text{Marginal}}\\
        &= \frac{\Prob{Y = y_j|X=x_i} \Prob{X = x_i}}{\sum_{k=1}^n\Prob{X = x_k,Y = y_j}}\\
        &= \frac{\Prob{Y = y_j|X=x_i} \Prob{X = x_i}}{\sum_{k=1}^n\Prob{Y = y_j|X = x_k}P(X = x_k)}\\
        \Rightarrow \Prob{(X|Y)} &= \frac{\Prob{Y|X} \Prob{X}}{\sum_x \Prob{X = x,Y}} = \frac{\text{Likelihood}\times \text{Prior}}{\text{Marginal}} 
    \end{align*}
    \subsection*{Continous Case}
        $$f(X|Y) = \frac{f(X,Y)}{g(Y)} = \frac{g(Y|X) f(X)}{\int g(Y|X)f(X) dX}$$

\subsection*{Bayesian Statistical Inference}
\begin{itemize}
    \item $L(\theta|y) = f(y|\theta)$: Likelihood
    \item $\pi(\theta)$: Prior
    \item $m(y) = p(y)$: Marginal distribution/Normalizing constant
\end{itemize}
$$\text{Posterior Distribution: } p(\theta|y) = \frac{f(y|\theta)\pi(\theta)}{\int p(\theta,y)d\theta} = \frac{f(y|\theta)\pi(\theta)}{\int f(y|\theta)\pi(\theta) d\theta} \propto f(y|\theta)\pi(\theta)$$
\subsection*{Predictive Distrubutions}
\begin{align*}
    \text{Prior Predictive Distribution} &\text{:} &p(y^{new}) &= \int_\theta f(y^{new}|\theta)\pi(\theta)d\theta\\
    \text{Posterior Predictive Distribution} &\text{:} &p(y^{new}|y^{old}) &= \int_\theta f(y^{new}|\theta,y^{old})p(\theta|y^{old})d\theta
\end{align*}
\subsection*{Bayes Theorem for Multiple Parameters $\Theta = \{\theta_1,...,\theta_q\}$}
$$\Prob{\Theta|\bm{y}} = \frac{\Prob{\bm{y}|\Theta}\Prob{\Theta}}{\Prob{\bm{y}}} \propto \Prob{\bm{y}|\Theta}\Prob{\Theta}$$

% Week 2 
\newpage
\section{Conjugate Priors}
\begin{center}
    \begin{tabular}{|l|c|l|}
        \hline
        Sample Distribution & Parameter & Prior\\
        \hline\hline
        Bernoulli$(\theta)$ & $\theta$ & Beta$(\alpha,\beta)$\\
        \hline
        Binomial$(n,\theta)$ & $\theta$ & Beta$(\alpha,\beta)$\\
        \hline
        Poisson$(\theta)$ & $\theta$ & Gamma$(\alpha,\beta)$\\
        \hline
        Exponential$(\theta)$ & $\theta$ & Gamma$(\alpha,\beta)$\\
        \hline
        Normal$(\mu,\sigma_0^2)$ & $\mu$ & Normal$(\mu_h,\sigma_h^2)$\\
        \hline
        Normal$(\mu_0,\sigma^2)$ & $\sigma^2$ & Inverse Gamma$(\alpha,\beta)$\\
        \hline
        Normal$(\mu_0,1/\tau)$ & $\tau = 1/\sigma^2$ & Gamma$(\alpha,\beta)$\\
        \hline
    \end{tabular}
\end{center}
\section{Normal Distribution Priors}
    \subsection*{Unknown $\mu$, known $\sigma^2$}
    \begin{itemize}
        \item Sample $\bm{y} \in \mathbb{R}^n$
        \item Prior $\mu \sim \text{Normal}(\mu_0,\sigma_0^2) = \text{Normal}\left(\mu_0,\frac{\sigma^2}{m}\right) \Rightarrow m = \sigma^2/\sigma_0^2$
        \item Posterior: $$\mu|\bm{y},\sigma^2 \sim \text{Normal}\left( \frac{m\mu_0 + n\bar{y}}{m+n} , \frac{\sigma^2}{m+n} \right) = \text{Normal}\left( (1-w)\mu_0 + w\bar{y} , \frac{\sigma^2}{m+n} \right)$$
            $w = \frac{m}{m+n}$
    \end{itemize}
    \subsection*{Unknown $\mu$, known $\tau$}
    \begin{itemize}
        \item Sample $\bm{y} \in \mathbb{R}^n$
        \item Prior $\mu \sim \text{Normal}\left(\mu_0,\frac{\sigma^2}{m}\right) = \text{Normal}\left(\mu_0,\frac{1}{\tau m}\right)$
        \item Posterior: $$\mu|\bm{y},\sigma^2 \sim \text{Normal}\left( \frac{m\mu_0 + n\bar{y}}{m+n} , \frac{1}{\tau(m+n)} \right) = \text{Normal}\left( (1-w)\mu_0 + w\bar{y} , \frac{1}{\tau(m+n)} \right)$$
            $w = \frac{m}{m+n}$
    \end{itemize}
    \subsection*{Unknown $\tau$, known $\mu$}
    \begin{itemize}
        \item Sample $\bm{y} \in \mathbb{R}^n$
        \item Prior $\tau \sim \text{Gamma}(\alpha,\beta)$
        \item Posterior: $$\tau|\bm{y},\mu \sim \text{Gamma}\left(\alpha + \frac{n}{2}, \beta + \frac{z^2}{2}\right)$$
            $z^2 = \sum_i(y_i - \mu)^2$
    \end{itemize}
    \subsection*{Unknown $\sigma^2$, known $\mu$}
    \begin{itemize}
        \item Sample $\bm{y} \in \mathbb{R}^n$
        \item Prior $\mu,\sigma^2 \sim \text{Normal}\left(\mu_0, \frac{\sigma^2}{\kappa}\right) \times \text{Inverse Gamma}(\alpha, \beta)$
        \item Posterior: $$\mu, \sigma^2|\bm{y} \sim \text{Normal}\left(\frac{\kappa\mu_0 + n\bar{y}}{\kappa + n},\frac{\sigma^2}{\kappa + n}\right) \times \text{Inverse Gamma}\left(\alpha + \frac{n}{2}, \beta + \frac{(n-1)s^2}{2} + \frac{\kappa n(\bar{y} - \mu_0)^2}{2(\kappa + n)}\right)$$
            $s^2 = \sum_i(y_i - \bar{y})^2/(n-1)$\\
            $\kappa = \sigma^2(\alpha-1)/\beta$ or given (seriously it's not written anywhere Ken, I'm just guessing here man)
    \end{itemize}

% Week 4
\section{Jeffrey's Prior}
    \subsection*{Definition}
    Jeffery Prior: $\pi_{JP}(\theta) \propto \sqrt{I(\theta|y)}$
    \\Fisher Information:
    $$I(\theta|y) = \Expect{\left( \frac{d \log f(y|\theta)}{d\theta} \right)^2} = -\Expect{ \frac{d^2 \log f(y|\theta)}{d\theta^2}}$$
    \subsection*{1:1 transformation of Jeffrey's Prior}
    $$\pi(\phi) = \pi_{JP}(\theta)\left| \frac{d \theta}{d \phi}\right|$$
    \subsection*{Jeffrey's Prior for Multivariate Parameter Vector}
    Gradient of log likelihood:
    $$ S(\bm{\theta}) = \begin{bmatrix*}
        \partial_{\theta_1} \log f(x)\\
        \vdots\\
        \partial_{\theta_n} \log f(x)
    \end{bmatrix*}$$
    Hessian Matrix of log likelihood:
    $$H(\bm{\theta}) = \text{Jacobian}\left[S(\bm{\theta})\right] = \begin{bmatrix*}
        \partial_{\theta_1}\partial_{\theta_1} \ln f(x) & \cdots & \partial_{\theta_1}\partial_{\theta_n} \ln f(x)\\
        \vdots & \ddots & \vdots\\
        \partial_{\theta_n}\partial_{\theta_1} \ln f(x) & \cdots & \partial_{\theta_n}\partial_{\theta_n} \ln f(x)
    \end{bmatrix*}$$
    Fisher Information: $I(\theta|x) = -\Expect{H(\theta)}$\\
    Jeffrey's Prior: $\pi_{JP} \propto \sqrt{\det(I(\theta|x))}$

\newpage
\section{Reference Prior}
    \subsection*{Kullback-Leibler  (KL)  divergence}
    A measure of the difference between two pmfs/pdfs. When $KL(f,g) = 0$, the two distributions are idetical.
        \subsubsection*{Properties}
        \begin{itemize}
            \item $KL(f,g) \neq KL(g,f)$
            \item $KL(fmg) \geq 0$
        \end{itemize}
        \subsubsection*{Continous parameter $x$}
        $$KL(f,g) = \int \ln\left[\frac{f(x)}{g(x)}\right]dx = \mathbb{E}_f[\log f(X)] - \mathbb{E}_g[\log f(X)]$$
        \subsubsection*{Discrete parameter $x$}
        $$KL(f,g) = \sum_{x\in X} \ln\left[\frac{f(x)}{g(x)}\right] = \mathbb{E}_f[\log f(X)] - \mathbb{E}_g[\log f(X)]$$
    \subsection*{Reference Prior} Read week 4 notes (2.2)

% Week 5
\section{Point Estimates}
	\subsection*{Components of Decision Theory with emphasis on parameter estimation}
		\begin{itemize}
			\item State Spcae $\Theta$, unknown true value $\theta \in \Theta$
			\item Action Space $\mathcal{A} \ni a$, sometimes $\mathcal{A} = \Theta$
			\item Sampling Distribution $f(\bm{y}| \theta)$
			\item Loss Function $\mathcal{L}(a|\theta)$
			\item Risk $R_\theta(a|\bm{y}) = \mathbb{E}_\theta[L(a|\theta,y)] = \int_{\theta \in \Theta} \mathcal{L}(a|\theta)p(\theta|\bm{y})d\theta$
			\item Bayes Estimator of a parameter $\hat{\theta}_{BE} = \argmin_{\hat{\theta} \in \Theta} R_\theta(\hat{\theta}|\bm{y})$
		\end{itemize}
	\subsection*{Common Loss Functions and Corresponding Estimators}
	 \begin{itemize}
		 \item \textbf{Squared Error Loss}: $\mathcal{L}(\theta|\hat{\theta}) = (\theta - \hat{\theta})^2$, Bayes Estimator $\hat{\theta} = \Expect{\theta|\bm{y}}$
		 \item \textbf{Absolute Error Loss}: $\mathcal{L}(\theta|\hat{\theta}) = |\theta - \hat{\theta}|$, Bayes Estimator $\hat{\theta} = \theta_{0.5}$, ($\Prob{\theta \leq \theta_{0.5}} = 0.5$)
		 \item \textbf{0-1 Loss}: $\mathcal{L}(\theta|\hat{\theta}) = I(\theta \neq \hat{\theta})$, Bayes Estimator $\hat{\theta}$ is the posterior mode
	 \end{itemize}

\newpage
\section{Interval Estimates}
	$$P\times 100\% \text{ Bayesian Credible interval} =[LB, UB] \text{ where } \int_{LB}^{UB} p(\theta,\bm{y})d\theta = P$$
	If loooking for one side-confidence bounds then $LB = -\infty$ or $UB = \infty$
	\subsection*{Symmetric Credicble Interval}
		$$[LB,UB] \text{ where } \Prob{\theta \leq LB} = \Prob{\theta \geq UB} = \alpha/2 = (1-P)/2$$
	\subsection*{Highest Posterior Density Interval (HPDI)}
		Credible Interval where all values outside the interval has a density smaller than any of the values in the interval.
	\subsection*{Credible Regions}
        $$\iint p(\theta_1, \theta_2| \bm{y})d\theta_1 d\theta_2 = 1 - \alpha = P$$ 

\section{Classic Hypothesis Testing}
    \begin{enumerate}
        \item Assume hypothesis $H_0$ is true
        \item Calculate test statistic $T(\bm{y}_{obs})$ based on observed sample data regarding to $H_0$ and $H_1$
        \item Conditional on $H_0$ being true, $p$-value $= \Prob{T(\bm{y})\text{ more extreme than } T(\bm{y}_{obs})|\theta, H_0}$
        \item Reject $H_0$ and accept $H_1$ for sufficiently small $p$-values, do not reject $H_0$ otherwise
    \end{enumerate}

\section{Bayesian Hypothesis Testing}
Suppose there are tow hypotheses about parameter $\theta$:
$$H_0:\theta \in \Theta_1 \quad H_1:\theta \in \Theta_1$$
where $\Theta_0 \cup \Theta_1 = \Theta$ and $\Theta_0 \cap \Theta_1 = \emptyset$.
\\~\\
The Bayesian approach specifies prior probabilities on each hypotheses:
\begin{align*}
    p_0 &= \Prob{H_0 \text{ is true}} = \Prob{\theta \in \Theta_0}\\
    p_1 &= \Prob{H_1 \text{ is true}} = \Prob{\theta \in \Theta_1}
\end{align*}
Where the posteriors are as follows:
\begin{align*}
    \Prob{H_0|\bm{y}} &= \Prob{\theta \in \Theta_0|\bm{y}}\\
    \Prob{H_1|\bm{y}} &= \Prob{\theta \in \Theta_1|\bm{y}}
\end{align*}
Where $\Prob{H_0|\bm{y}} + \Prob{H_1|\bm{y}} = 1$
\newpage
\subsection*{Simple}
\begin{itemize}
    \item Single parameter values $H_0:\theta = \theta_0 \quad H_1:\theta = \theta_1$
    \item Posteriors:
        \begin{align*}
            \Prob{H_0|\bm{y}} &= \Prob{\theta = \theta_0|\bm{y}} = \frac{f(\bm{y}|\theta_0)p_0}{m(\bm{y})} = \frac{f(\bm{y}|\theta_0)p_0}{f(\bm{y}|\theta_0)p_0 + f(\bm{y}|\theta_1)p_1}\\
            \Prob{H_1|\bm{y}} &= \Prob{\theta = \theta_1|\bm{y}} = \frac{f(\bm{y}|\theta_1)p_1}{m(\bm{y})} = \frac{f(\bm{y}|\theta_1)p_1}{f(\bm{y}|\theta_0)p_0 + f(\bm{y}|\theta_1)p_1}\\
            \Prob{H_0|\bm{y}} &= 1 - \Prob{H_1|\bm{y}}
        \end{align*}
    \item posterior odds of $H_0$ against $H_1$:
        $$\frac{\Prob{H_0|\bm{y}}}{\Prob{H_1|\bm{y}}} = \frac{f(\bm{y}|\theta_0)p_0}{f(\bm{y}|\theta_1)p_1}$$
\end{itemize}
\subsection*{Composite}
\begin{itemize}
    \item Single parameter values $H_0:\theta \in \Theta_0 \quad H_1:\theta \in \Theta_1$
    \item Prior: $p_i = \int_{\theta \in \Theta_i}\pi(\theta) d\theta$
    \item Posteriors:
        \begin{align*}
            \Prob{H_i|\bm{y}} &= \frac{p(\bm{y},H_i)}{m(\bm{y})} = \frac{p_i\ p(\bm{y}|H_i)}{m(\bm{y})} = \frac{p_i\int p(\bm{y},\theta|H_i)d\theta}{m(\bm{y})} = \frac{p_i\int f(\bm{y}|\theta)\pi(\theta|H_i) d\theta}{m(\bm{y})}\\
            &= \frac{p_i\int_{\theta \in \Theta_i} f(\bm{y}|\theta)\frac{\pi(\theta)}{p_i} d\theta}{m(\bm{y})} = \frac{\int_{\theta \in \Theta_i} f(\bm{y}|\theta)\pi(\theta) d\theta}{m(\bm{y})} = \int_{\theta \in \Theta_i} p(\theta|\bm{y}) d\theta = \Prob{\theta \in \Theta_i|\bm{y}}
        \end{align*}
    \item posterior odds of $H_0$ against $H_1$:
        $$\frac{\Prob{H_0|\bm{y}}}{\Prob{H_1|\bm{y}}} = \frac{\int_{\theta \in \Theta_0}f(\bm{y}|\theta) \pi(\theta)d\theta}{\int_{\theta \in \Theta_1}f(\bm{y}|\theta) \pi(\theta)d\theta} = \frac{\Prob{\theta \in \Theta_0|\bm{y}}}{\Prob{\theta \in \Theta_1|\bm{y}}} = \frac{\Prob{\theta \in \Theta_0|\bm{y}}}{1-\Prob{\theta \in \Theta_0|\bm{y}}}$$
\end{itemize}
\subsection*{Multiple models}
\begin{itemize}
    \item Set of models $M_1,...,M_K$
    \item $H_i$: The correct model is $M_i$
    $$\Prob{H_i|\bm{y}} = \frac{\Prob{H_i, \bm{y}}}{\Prob{\bm{y}}} = \frac{\Prob{H_i, \bm{y}}}{\sum_{j=1}^K \Prob{H_j,\bm{y}}}$$
\end{itemize}
\newpage
\section*{Bayes Factor}
\begin{itemize}
    \item Prior odds for $H_0$ against $H_1 = {p_0}/{p_1}$
    \item Posterior odds for $H_0$ against $H_1 = \Prob{H_0|\bm{y}}/\Prob{H_1|\bm{y}}$
    \item Bayes Factor for $H_0$ against $H_1$:
        \begin{align*}
            BF_{01} &= \frac{\text{Posterior odds for $H_0$ against $H_1$}}{\text{Prior odds for $H_0$ against $H_1$}} = \frac{\Prob{H_0|\bm{y}}/\Prob{H_1|\bm{y}}}{p_0/p_1}\\
            BF_{10} &= \frac{1}{BF_{01}}
        \end{align*}
\end{itemize}
\begin{center}
    \begin{tabular}{|c|r|}
        \hline
        $BF_{ij}$ & Interpretation\\
        \hline\hline
        $<3$ & No edvidence for $H_i$ over $H_j$\\
        $>3$ & Positive edvidence for $H_i$\\
        $>20$ & Strong edvidence for $H_i$\\
        $>150$ & Very strong edvidence for $H_i$\\\hline
    \end{tabular}
\end{center}
\end{document}